{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python382jvsc74a57bd0a7d41030fb8a9e888cd20c6c924d3b86165140a58bcf0faa8b2fd9fdb4864645",
   "display_name": "Python 3.8.2 64-bit ('env')"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import tensorflow as tf\n",
    "\n",
    "X_train = np.load('./prepared/X_train.npy').astype('float32')\n",
    "y_train = np.load('./prepared/y_train.npy').astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a representative dataset for quantization \n",
    "def representative_dataset():\n",
    "    for i in range(300):\n",
    "      data = X_train[i]\n",
    "      yield [data.astype(np.float32)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"mlp_1 = tf.keras.Sequential([\\n    tf.keras.layers.Input(X_train.shape[1]),\\n    tf.keras.layers.Dense(25, activation = 'sigmoid'),\\n    tf.keras.layers.Dense(25, activation = 'sigmoid'),\\n    tf.keras.layers.Dense(1, activation = 'sigmoid')\\n])\\n\\nmlp_1.compile(optimizer = 'adam', loss = tf.keras.losses.BinaryCrossentropy(from_logits = True), metrics = ['accuracy'])\\nmlp_1.fit(X_train, y_train, epochs = 100)\\n# Convert to tflite model, alongside interger quantization with fp32 fallback\\nconverter = tf.lite.TFLiteConverter.from_keras_model(mlp_1) # path to the SavedModel directory\\nmlp_1 = converter.convert()\\n\\n# Save the model.\\nwith open('models/sigmoid_2layer_25neurons.tflite', 'wb') as f:\\n  f.write(mlp_1)\""
      ]
     },
     "metadata": {},
     "execution_count": 167
    }
   ],
   "source": [
    "# Train first MLP with 25 neurons in each hidden layer\n",
    "mlp_25_neurons = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(X_train.shape[1]),\n",
    "    tf.keras.layers.Dense(25, activation = 'sigmoid'),\n",
    "    tf.keras.layers.Dense(25, activation = 'sigmoid'),\n",
    "    tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
    "])\n",
    "\n",
    "# Compile using ADAM optimizer, and cross entropy losss\n",
    "mlp_25_neurons.compile(optimizer = 'adam', \n",
    "loss = tf.keras.losses.BinaryCrossentropy(from_logits = True), metrics = ['accuracy'])\n",
    "# and train for 100 epochs\n",
    "mlp_25_neurons.fit(X_train, y_train, epochs = 100)\n",
    "# convert to tflite format\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(mlp_25_neurons)\n",
    "mlp_25_neurons = converter.convert()\n",
    "\n",
    "# Save the model.\n",
    "with open('models/sigmoid_2layer_25neurons.tflite', 'wb') as f:\n",
    "  f.write(mlp_25_neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"mlp_2 = tf.keras.Sequential([\\n    tf.keras.layers.Input(X_train.shape[1]),\\n    tf.keras.layers.Dense(50, activation = 'sigmoid'),\\n    tf.keras.layers.Dense(50, activation = 'sigmoid'),\\n    tf.keras.layers.Dense(1, activation = 'sigmoid')\\n])\\n\\n\\nmlp_2.compile(optimizer = 'adam', loss = tf.keras.losses.BinaryCrossentropy(from_logits = True), metrics = ['accuracy'])\\nmlp_2.fit(X_train, y_train, epochs = 100)\\n# Convert to tflite model, alongside interger quantization with fp32 fallback\\nconverter = tf.lite.TFLiteConverter.from_keras_model(mlp_2) # path to the SavedModel directory\\nmlp_2 = converter.convert()\\n\\n# Save the model.\\nwith open('models/sigmoid_2layer_50neurons.tflite', 'wb') as f:\\n  f.write(mlp_2)\""
      ]
     },
     "metadata": {},
     "execution_count": 168
    }
   ],
   "source": [
    "mlp_50_neurons = tf.keras.Sequential([\n",
    "    fkeras.layers.Input(X_train.shape[1]),\n",
    "    tf.keras.layers.Dense(50, activation = 'sigmoid'),\n",
    "    tf.keras.layers.Dense(50, activation = 'sigmoid'),\n",
    "    tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
    "])\n",
    "\n",
    "\n",
    "mlp_50_neurons.compile(optimizer = 'adam', loss = tf.keras.losses.BinaryCrossentropy(from_logits = True), metrics = ['accuracy'])\n",
    "mlp_50_neurons.fit(X_train, y_train, epochs = 100)\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(mlp_50_neurons) # path to the SavedModel directory\n",
    "mlp_50_neurons = converter.convert()\n",
    "\n",
    "# Save the model.\n",
    "with open('models/sigmoid_2layer_50neurons.tflite', 'wb') as f:\n",
    "  f.write(mlp_50_neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"mlp_3 = tf.keras.Sequential([\\n    tf.keras.layers.Input(X_train.shape[1]),\\n    tf.keras.layers.Dense(75, activation = 'sigmoid'),\\n    tf.keras.layers.Dense(75, activation = 'sigmoid'),\\n    tf.keras.layers.Dense(1, activation = 'sigmoid')\\n])\\n\\n\\nmlp_3.compile(optimizer = 'adam', loss = tf.keras.losses.BinaryCrossentropy(from_logits = True), metrics = ['accuracy'])\\nmlp_3.fit(X_train, y_train, epochs = 100)\\n# Convert to tflite model, alongside interger quantization with fp32 fallback\\nconverter = tf.lite.TFLiteConverter.from_keras_model(mlp_3) # path to the SavedModel directory\\nmlp_3 = converter.convert()\\n\\n# Save the model.\\nwith open('models/sigmoid_2layer_75neurons.tflite', 'wb') as f:\\n  f.write(mlp_3)\""
      ]
     },
     "metadata": {},
     "execution_count": 169
    }
   ],
   "source": [
    "mlp_75_neurons = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(X_train.shape[1]),\n",
    "    tf.keras.layers.Dense(75, activation = 'sigmoid'),\n",
    "    tf.keras.layers.Dense(75, activation = 'sigmoid'),\n",
    "    tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
    "])\n",
    "\n",
    "\n",
    "mlp_75_neurons.compile(optimizer = 'adam', loss = tf.keras.losses.BinaryCrossentropy(from_logits = True), metrics = ['accuracy'])\n",
    "mlp_75_neurons.fit(X_train, y_train, epochs = 100)\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(mlp_75_neurons) # path to the SavedModel directory\n",
    "mlp_75_neurons = converter.convert()\n",
    "\n",
    "# Save the model.\n",
    "with open('models/sigmoid_2layer_75neurons.tflite', 'wb') as f:\n",
    "  f.write(mlp_75_neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"mlp_4 = tf.keras.Sequential([\\n    tf.keras.layers.Input(X_train.shape[1]),\\n    tf.keras.layers.Dense(125, activation = 'sigmoid'),\\n    tf.keras.layers.Dense(125, activation = 'sigmoid'),\\n    tf.keras.layers.Dense(1, activation = 'sigmoid')\\n])\\n\\n\\nmlp_4.compile(optimizer = 'adam', loss = tf.keras.losses.BinaryCrossentropy(from_logits = True), metrics = ['accuracy'])\\nmlp_4.fit(X_train, y_train, epochs = 100)\\nconverter = tf.lite.TFLiteConverter.from_keras_model(mlp_4) # path to the SavedModel directory\\nmlp_4 = converter.convert()\\n\\n# Save the model.\\nwith open('models/sigmoid_2layer_125neurons.tflite', 'wb') as f:\\n  f.write(mlp_4)\""
      ]
     },
     "metadata": {},
     "execution_count": 170
    }
   ],
   "source": [
    "mlp_125_neurons = tf.keras.Sequentia([\n",
    "    tf.keras.layers.Input(Xf.keras.layers.Input(X_train.shape[1]),\n",
    "    tf.keras.layers.Dense(125, activation = 'sigmoid'),\n",
    "    tf.keras.layers.Dense(125, activation = 'sigmoid'),\n",
    "    tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
    "])\n",
    "\n",
    "\n",
    "mlp_125_neurons.compile(optimizer = 'adam', loss = tf.keras.losses.BinaryCrossentropy(from_logits = True), metrics = ['accuracy'])\n",
    "mlp_125_neurons.fit(X_train, y_train, epochs = 100)\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(mlp_125_neurons) # path to the SavedModel directory\n",
    "mlp_125_neurons = converter.convert()\n",
    "\n",
    "# Save the model.\n",
    "with open('models/sigmoid_2layer_125neurons.tflite', 'wb') as f:\n",
    "  f.write(mlp_125_neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"mlp_5 = tf.keras.Sequential([\\n    tf.keras.layers.Input(X_train.shape[1]),\\n    tf.keras.layers.Dense(150, activation = 'sigmoid'),\\n    tf.keras.layers.Dense(150, activation = 'sigmoid'),\\n    tf.keras.layers.Dense(1, activation = 'sigmoid')\\n])\\n\\nmlp_5.compile(optimizer = 'adam', loss = tf.keras.losses.BinaryCrossentropy(from_logits = True), metrics = ['accuracy'])\\nmlp_5.fit(X_train, y_train, epochs = 100)\\nconverter = tf.lite.TFLiteConverter.from_keras_model(mlp_5) # path to the SavedModel directory\\nmlp_5 = converter.convert()\\n\\n# Save the model.\\nwith open('models/sigmoid_2layer_150neurons.tflite', 'wb') as f:\\n  f.write(mlp_5)\""
      ]
     },
     "metadata": {},
     "execution_count": 171
    }
   ],
   "source": [
    "mlp_150_neurons = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(X_train.shape[1]),\n",
    "    tf.keras.layers.Dense(150, activation = 'sigmoid'),\n",
    "    tf.keras.layers.Dense(150, activation = 'sigmoid'),\n",
    "    tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
    "])\n",
    "\n",
    "mlp_150_neurons.compile(optimizer = 'adam', loss = tf.keras.losses.BinaryCrossentropy(from_logits = True), metrics = ['accuracy'])\n",
    "mlp_150_neurons.fit(X_train, y_train, epochs = 100)\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(mlp_150_neurons) # path to the SavedModel directory\n",
    "mlp_150_neurons = converter.convert()\n",
    "\n",
    "# Save the model.\n",
    "with open('models/sigmoid_2layer_150neurons.tflite', 'wb') as f:\n",
    "  f.write(mlp_150_neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"mlp_6 = tf.keras.Sequential([\\n    tf.keras.layers.Input(X_train.shape[1]),\\n    tf.keras.layers.Dense(175, activation = 'sigmoid'),\\n    tf.keras.layers.Dense(175, activation = 'sigmoid'),\\n    tf.keras.layers.Dense(1, activation = 'sigmoid')\\n])\\n\\nmlp_6.compile(optimizer = 'adam', loss = tf.keras.losses.BinaryCrossentropy(from_logits = True), metrics = ['accuracy'])\\nmlp_6.fit(X_train, y_train, epochs = 100)\\nconverter = tf.lite.TFLiteConverter.from_keras_model(mlp_6) # path to the SavedModel directory\\nmlp_6 = converter.convert()\\n\\n# Save the model.\\nwith open('models/sigmoid_2layer_175neurons.tflite', 'wb') as f:\\n  f.write(mlp_6)\""
      ]
     },
     "metadata": {},
     "execution_count": 172
    }
   ],
   "source": [
    "mlp_175_neurons = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(X_train.shape[1]),\n",
    "    tf.keras.layers.Dense(175, activation = 'sigmoid'),\n",
    "    tf.keras.layers.Dense(175, activation = 'sigmoid'),\n",
    "    tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
    "])\n",
    "\n",
    "mlp_175_neurons.compile(optimizer = 'adam', loss = tf.keras.losses.BinaryCrossentropy(from_logits = True), metrics = ['accuracy'])\n",
    "mlp_175_neurons.fit(X_train, y_train, epochs = 100)\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(mlp_175_neurons) # path to the SavedModel directory\n",
    "mlp_175_neurons = converter.convert()\n",
    "\n",
    "# Save the model.\n",
    "with open('models/sigmoid_2layer_175neurons.tflite', 'wb') as f:\n",
    "  f.write(mlp_175_neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/100\n",
      "3329/3329 [==============================] - 5s 1ms/step - loss: 0.2149 - accuracy: 0.9159\n",
      "Epoch 2/100\n",
      "3329/3329 [==============================] - 4s 1ms/step - loss: 0.1172 - accuracy: 0.9514\n",
      "Epoch 3/100\n",
      "3329/3329 [==============================] - 4s 1ms/step - loss: 0.1022 - accuracy: 0.9588\n",
      "Epoch 4/100\n",
      "3329/3329 [==============================] - 4s 1ms/step - loss: 0.0978 - accuracy: 0.9588\n",
      "Epoch 5/100\n",
      "3329/3329 [==============================] - 4s 1ms/step - loss: 0.0934 - accuracy: 0.9607\n",
      "Epoch 6/100\n",
      "3329/3329 [==============================] - 5s 1ms/step - loss: 0.0901 - accuracy: 0.9623\n",
      "Epoch 7/100\n",
      "3329/3329 [==============================] - 4s 1ms/step - loss: 0.0912 - accuracy: 0.9606\n",
      "Epoch 8/100\n",
      "3329/3329 [==============================] - 4s 1ms/step - loss: 0.0875 - accuracy: 0.9618\n",
      "Epoch 9/100\n",
      "3329/3329 [==============================] - 5s 2ms/step - loss: 0.0889 - accuracy: 0.9620\n",
      "Epoch 10/100\n",
      "3329/3329 [==============================] - 5s 1ms/step - loss: 0.0816 - accuracy: 0.9642\n",
      "Epoch 11/100\n",
      "3329/3329 [==============================] - 4s 1ms/step - loss: 0.0812 - accuracy: 0.9659\n",
      "Epoch 12/100\n",
      "3329/3329 [==============================] - 4s 1ms/step - loss: 0.0764 - accuracy: 0.9676\n",
      "Epoch 13/100\n",
      "3329/3329 [==============================] - 4s 1ms/step - loss: 0.0729 - accuracy: 0.9687\n",
      "Epoch 14/100\n",
      "3329/3329 [==============================] - 4s 1ms/step - loss: 0.0744 - accuracy: 0.9689\n",
      "Epoch 15/100\n",
      "3329/3329 [==============================] - 4s 1ms/step - loss: 0.0739 - accuracy: 0.9687\n",
      "Epoch 16/100\n",
      "3329/3329 [==============================] - 4s 1ms/step - loss: 0.0748 - accuracy: 0.9680\n",
      "Epoch 17/100\n",
      "3329/3329 [==============================] - 4s 1ms/step - loss: 0.0717 - accuracy: 0.9689\n",
      "Epoch 18/100\n",
      "3329/3329 [==============================] - 5s 1ms/step - loss: 0.0726 - accuracy: 0.9693\n",
      "Epoch 19/100\n",
      "3329/3329 [==============================] - 5s 1ms/step - loss: 0.0723 - accuracy: 0.9686\n",
      "Epoch 20/100\n",
      "3329/3329 [==============================] - 5s 1ms/step - loss: 0.0765 - accuracy: 0.9664\n",
      "Epoch 21/100\n",
      "3329/3329 [==============================] - 4s 1ms/step - loss: 0.0731 - accuracy: 0.9686\n",
      "Epoch 22/100\n",
      "3329/3329 [==============================] - 4s 1ms/step - loss: 0.0731 - accuracy: 0.9687\n",
      "Epoch 23/100\n",
      "3329/3329 [==============================] - 4s 1ms/step - loss: 0.0717 - accuracy: 0.9697\n",
      "Epoch 24/100\n",
      "3329/3329 [==============================] - 4s 1ms/step - loss: 0.0739 - accuracy: 0.9690\n",
      "Epoch 25/100\n",
      "3329/3329 [==============================] - 4s 1ms/step - loss: 0.0737 - accuracy: 0.9683\n",
      "Epoch 26/100\n",
      "3329/3329 [==============================] - 4s 1ms/step - loss: 0.0720 - accuracy: 0.9690\n",
      "Epoch 27/100\n",
      "3329/3329 [==============================] - 4s 1ms/step - loss: 0.0703 - accuracy: 0.9700\n",
      "Epoch 28/100\n",
      "3329/3329 [==============================] - 4s 1ms/step - loss: 0.0682 - accuracy: 0.9707\n",
      "Epoch 29/100\n",
      "3329/3329 [==============================] - 4s 1ms/step - loss: 0.0742 - accuracy: 0.9690\n",
      "Epoch 30/100\n",
      "3329/3329 [==============================] - 4s 1ms/step - loss: 0.0706 - accuracy: 0.9687\n",
      "Epoch 31/100\n",
      "3329/3329 [==============================] - 4s 1ms/step - loss: 0.0672 - accuracy: 0.9709\n",
      "Epoch 32/100\n",
      "3329/3329 [==============================] - 4s 1ms/step - loss: 0.0735 - accuracy: 0.9687\n",
      "Epoch 33/100\n",
      "3329/3329 [==============================] - 5s 1ms/step - loss: 0.0764 - accuracy: 0.9654\n",
      "Epoch 34/100\n",
      "3329/3329 [==============================] - 4s 1ms/step - loss: 0.0761 - accuracy: 0.9674\n",
      "Epoch 35/100\n",
      "3329/3329 [==============================] - 4s 1ms/step - loss: 0.0683 - accuracy: 0.9708\n",
      "Epoch 36/100\n",
      "3329/3329 [==============================] - 4s 1ms/step - loss: 0.0703 - accuracy: 0.9698\n",
      "Epoch 37/100\n",
      "3329/3329 [==============================] - 4s 1ms/step - loss: 0.0718 - accuracy: 0.9707\n",
      "Epoch 38/100\n",
      "3329/3329 [==============================] - 4s 1ms/step - loss: 0.0687 - accuracy: 0.9696\n",
      "Epoch 39/100\n",
      "3329/3329 [==============================] - 4s 1ms/step - loss: 0.0676 - accuracy: 0.9710\n",
      "Epoch 40/100\n",
      "3329/3329 [==============================] - 4s 1ms/step - loss: 0.0677 - accuracy: 0.9712\n",
      "Epoch 41/100\n",
      "3329/3329 [==============================] - 4s 1ms/step - loss: 0.0660 - accuracy: 0.9714\n",
      "Epoch 42/100\n",
      "3329/3329 [==============================] - 4s 1ms/step - loss: 0.0664 - accuracy: 0.9704\n",
      "Epoch 43/100\n",
      "3329/3329 [==============================] - 4s 1ms/step - loss: 0.0656 - accuracy: 0.9711\n",
      "Epoch 44/100\n",
      "3329/3329 [==============================] - 4s 1ms/step - loss: 0.0667 - accuracy: 0.9712\n",
      "Epoch 45/100\n",
      "3329/3329 [==============================] - 4s 1ms/step - loss: 0.0677 - accuracy: 0.9703\n",
      "Epoch 46/100\n",
      "3329/3329 [==============================] - 4s 1ms/step - loss: 0.0653 - accuracy: 0.9712\n",
      "Epoch 47/100\n",
      "3329/3329 [==============================] - 4s 1ms/step - loss: 0.0659 - accuracy: 0.9704\n",
      "Epoch 48/100\n",
      "3329/3329 [==============================] - 4s 1ms/step - loss: 0.0673 - accuracy: 0.9705\n",
      "Epoch 49/100\n",
      "3329/3329 [==============================] - 4s 1ms/step - loss: 0.0728 - accuracy: 0.9692\n",
      "Epoch 50/100\n",
      "3329/3329 [==============================] - 4s 1ms/step - loss: 0.0676 - accuracy: 0.9709\n",
      "Epoch 51/100\n",
      "3329/3329 [==============================] - 4s 1ms/step - loss: 0.0664 - accuracy: 0.9704\n",
      "Epoch 52/100\n",
      "3329/3329 [==============================] - 3s 1ms/step - loss: 0.0736 - accuracy: 0.9713\n",
      "Epoch 53/100\n",
      "3329/3329 [==============================] - 3s 1ms/step - loss: 0.0641 - accuracy: 0.9726\n",
      "Epoch 54/100\n",
      "3329/3329 [==============================] - 3s 1ms/step - loss: 0.0655 - accuracy: 0.9716\n",
      "Epoch 55/100\n",
      "3329/3329 [==============================] - 3s 1ms/step - loss: 0.0646 - accuracy: 0.9724\n",
      "Epoch 56/100\n",
      "3329/3329 [==============================] - 3s 1ms/step - loss: 0.0651 - accuracy: 0.9715\n",
      "Epoch 57/100\n",
      "3329/3329 [==============================] - 3s 1ms/step - loss: 0.0662 - accuracy: 0.9710\n",
      "Epoch 58/100\n",
      "3329/3329 [==============================] - 3s 1ms/step - loss: 0.0658 - accuracy: 0.9722\n",
      "Epoch 59/100\n",
      "3329/3329 [==============================] - 3s 1ms/step - loss: 0.0639 - accuracy: 0.9729\n",
      "Epoch 60/100\n",
      "3329/3329 [==============================] - 3s 1ms/step - loss: 0.0638 - accuracy: 0.9719\n",
      "Epoch 61/100\n",
      "3329/3329 [==============================] - 3s 1ms/step - loss: 0.0630 - accuracy: 0.9725\n",
      "Epoch 62/100\n",
      "3329/3329 [==============================] - 3s 1ms/step - loss: 0.0607 - accuracy: 0.9733\n",
      "Epoch 63/100\n",
      "3329/3329 [==============================] - 3s 1ms/step - loss: 0.0618 - accuracy: 0.9734\n",
      "Epoch 64/100\n",
      "3329/3329 [==============================] - 3s 1ms/step - loss: 0.0708 - accuracy: 0.9723\n",
      "Epoch 65/100\n",
      "3329/3329 [==============================] - 3s 1ms/step - loss: 0.0635 - accuracy: 0.9719\n",
      "Epoch 66/100\n",
      "3329/3329 [==============================] - 4s 1ms/step - loss: 0.0643 - accuracy: 0.9727\n",
      "Epoch 67/100\n",
      "3329/3329 [==============================] - 3s 1ms/step - loss: 0.0642 - accuracy: 0.9727\n",
      "Epoch 68/100\n",
      "3329/3329 [==============================] - 3s 1ms/step - loss: 0.0678 - accuracy: 0.9722\n",
      "Epoch 69/100\n",
      "3329/3329 [==============================] - 3s 1ms/step - loss: 0.0627 - accuracy: 0.9729\n",
      "Epoch 70/100\n",
      "3329/3329 [==============================] - 3s 1ms/step - loss: 0.0612 - accuracy: 0.9739\n",
      "Epoch 71/100\n",
      "3329/3329 [==============================] - 3s 1ms/step - loss: 0.0610 - accuracy: 0.9741\n",
      "Epoch 72/100\n",
      "3329/3329 [==============================] - 3s 1ms/step - loss: 0.0631 - accuracy: 0.9731\n",
      "Epoch 73/100\n",
      "3329/3329 [==============================] - 3s 1ms/step - loss: 0.0655 - accuracy: 0.9727\n",
      "Epoch 74/100\n",
      "3329/3329 [==============================] - 3s 1ms/step - loss: 0.0625 - accuracy: 0.9734\n",
      "Epoch 75/100\n",
      "3329/3329 [==============================] - 3s 1ms/step - loss: 0.0627 - accuracy: 0.9729\n",
      "Epoch 76/100\n",
      "3329/3329 [==============================] - 3s 1ms/step - loss: 0.0611 - accuracy: 0.9737\n",
      "Epoch 77/100\n",
      "3329/3329 [==============================] - 3s 1ms/step - loss: 0.0630 - accuracy: 0.9724\n",
      "Epoch 78/100\n",
      "3329/3329 [==============================] - 3s 1ms/step - loss: 0.0614 - accuracy: 0.9732\n",
      "Epoch 79/100\n",
      "3329/3329 [==============================] - 3s 1ms/step - loss: 0.0618 - accuracy: 0.9730\n",
      "Epoch 80/100\n",
      "3329/3329 [==============================] - 4s 1ms/step - loss: 0.0610 - accuracy: 0.9738\n",
      "Epoch 81/100\n",
      "3329/3329 [==============================] - 4s 1ms/step - loss: 0.0586 - accuracy: 0.9747\n",
      "Epoch 82/100\n",
      "3329/3329 [==============================] - 4s 1ms/step - loss: 0.0629 - accuracy: 0.9727\n",
      "Epoch 83/100\n",
      "3329/3329 [==============================] - 5s 1ms/step - loss: 0.0669 - accuracy: 0.9715\n",
      "Epoch 84/100\n",
      "3329/3329 [==============================] - 4s 1ms/step - loss: 0.0612 - accuracy: 0.9736\n",
      "Epoch 85/100\n",
      "3329/3329 [==============================] - 4s 1ms/step - loss: 0.0594 - accuracy: 0.9742\n",
      "Epoch 86/100\n",
      "3329/3329 [==============================] - 4s 1ms/step - loss: 0.0619 - accuracy: 0.9730\n",
      "Epoch 87/100\n",
      "3329/3329 [==============================] - 3s 1ms/step - loss: 0.0603 - accuracy: 0.9739\n",
      "Epoch 88/100\n",
      "3329/3329 [==============================] - 3s 1ms/step - loss: 0.0609 - accuracy: 0.9732\n",
      "Epoch 89/100\n",
      "3329/3329 [==============================] - 3s 1ms/step - loss: 0.0602 - accuracy: 0.9746\n",
      "Epoch 90/100\n",
      "3329/3329 [==============================] - 3s 1ms/step - loss: 0.1772 - accuracy: 0.9726\n",
      "Epoch 91/100\n",
      "3329/3329 [==============================] - 4s 1ms/step - loss: 0.0589 - accuracy: 0.9748\n",
      "Epoch 92/100\n",
      "3329/3329 [==============================] - 4s 1ms/step - loss: 0.0595 - accuracy: 0.9745\n",
      "Epoch 93/100\n",
      "3329/3329 [==============================] - 4s 1ms/step - loss: 0.0618 - accuracy: 0.9730\n",
      "Epoch 94/100\n",
      "3329/3329 [==============================] - 4s 1ms/step - loss: 0.0603 - accuracy: 0.9736\n",
      "Epoch 95/100\n",
      "3329/3329 [==============================] - 4s 1ms/step - loss: 0.0592 - accuracy: 0.9743\n",
      "Epoch 96/100\n",
      "3329/3329 [==============================] - 5s 1ms/step - loss: 0.0609 - accuracy: 0.9738\n",
      "Epoch 97/100\n",
      "3329/3329 [==============================] - 4s 1ms/step - loss: 0.0604 - accuracy: 0.9744\n",
      "Epoch 98/100\n",
      "3329/3329 [==============================] - 4s 1ms/step - loss: 0.0605 - accuracy: 0.9733\n",
      "Epoch 99/100\n",
      "3329/3329 [==============================] - 4s 1ms/step - loss: 0.0602 - accuracy: 0.9742\n",
      "Epoch 100/100\n",
      "3329/3329 [==============================] - 4s 1ms/step - loss: 0.0607 - accuracy: 0.9738\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\Tom\\AppData\\Local\\Temp\\tmpg7735pex\\assets\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\Tom\\AppData\\Local\\Temp\\tmpg7735pex\\assets\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\Tom\\AppData\\Local\\Temp\\tmpnstmzv4u\\assets\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\Tom\\AppData\\Local\\Temp\\tmpnstmzv4u\\assets\n"
     ]
    }
   ],
   "source": [
    "# Create baseline model for evaluation on test set\n",
    "test_mlp = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(X_train.shape[1]),\n",
    "    tf.keras.layers.Dense(150, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(150, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
    "])\n",
    "\n",
    "test_mlp.compile(optimizer = 'adam', loss = tf.keras.losses.BinaryCrossentropy(from_logits = True), metrics = ['accuracy'])\n",
    "test_mlp.fit(X_train, y_train, epochs = 100)\n",
    "\n",
    "# And create a copy for quantization\n",
    "model_for_quantization = tf.keras.models.clone_model(test_mlp)\n",
    "model_for_quantization.set_weights(test_mlp.get_weights())\n",
    "\n",
    "# Save baseline model to disk\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(test_mlp)\n",
    "converted_test_mlp = converter.convert()\n",
    "with open('models/baseline.tflite', 'wb') as f:\n",
    "  f.write(converted_test_mlp)\n",
    "\n",
    "\n",
    "# quantize copied model to int8, with fp32 fallback\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model_for_quantization) # path to the SavedModel directory\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset\n",
    "model_for_quantization = converter.convert()\n",
    "\n",
    "# Save the model.\n",
    "with open('models/quantized.tflite', 'wb') as f:\n",
    "  f.write(model_for_quantization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2996/2996 [==============================] - 8s 2ms/step - loss: 0.7492 - accuracy: 0.8868 - val_loss: 0.3498 - val_accuracy: 0.9396\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\Tom\\AppData\\Local\\Temp\\tmp53nd3ui6\\assets\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\Tom\\AppData\\Local\\Temp\\tmp53nd3ui6\\assets\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\Tom\\AppData\\Local\\Temp\\tmpxqdkpyn0\\assets\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\Tom\\AppData\\Local\\Temp\\tmpxqdkpyn0\\assets\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "cluster_weights = tfmot.clustering.keras.cluster_weights\n",
    "CentroidInitialization = tfmot.clustering.keras.CentroidInitialization\n",
    "\n",
    "# reduce number of weights stored by 50% (75 clusters from 150 neurons) and use linear initialization\n",
    "clustering_params = {\n",
    "  'number_of_clusters': 75,\n",
    "  'cluster_centroids_init': CentroidInitialization.LINEAR\n",
    "}\n",
    "\n",
    "# Create another copy of the baseline model \n",
    "model_for_clustering = tf.keras.models.clone_model(test_mlp)\n",
    "model_for_clustering.set_weights(test_mlp.get_weights())\n",
    "\n",
    "# And perform weight clustering\n",
    "clustered_model = cluster_weights(model_for_clustering, **clustering_params)\n",
    "# Use smaller learning rate for fine-tuning clustered model\n",
    "opt = tf.keras.optimizers.Adam(learning_rate = 1e-5)\n",
    "\n",
    "clustered_model.compile(loss = tf.keras.losses.BinaryCrossentropy(from_logits = True), optimizer = opt, metrics = ['accuracy'])\n",
    "# Fine-tune model\n",
    "clustered_model.fit(\n",
    "  X_train,\n",
    "  y_train,\n",
    "  epochs=1,\n",
    "  validation_split=0.1)\n",
    "\n",
    "final_model = tfmot.clustering.keras.strip_clustering(clustered_model)\n",
    "\n",
    "# Create copy of clustered model, for evaluating additional quantization\n",
    "final_quantized = tf.keras.models.clone_model(final_model)\n",
    "final_quantized.set_weights(final_model.get_weights())\n",
    "\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(final_model) # path to the SavedModel directory\n",
    "final_model = converter.convert()\n",
    "# Save the model.\n",
    "with open('models/clustered.tflite', 'wb') as f:\n",
    "  f.write(final_model)\n",
    "\n",
    "\n",
    "# combined clustering and quantization\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(final_quantized) # path to the SavedModel directory\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset\n",
    "final_quantized = converter.convert()\n",
    "with open('models/clustered_quantized.tflite', 'wb') as f:\n",
    "  f.write(final_quantized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "c:\\Users\\Tom\\HikariIDS\\env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:2281: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
      "  warnings.warn('`layer.add_variable` is deprecated and '\n",
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "prune_low_magnitude_dense_45 (None, 150)               31352     \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_dense_46 (None, 150)               45152     \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_dense_47 (None, 1)                 303       \n",
      "=================================================================\n",
      "Total params: 76,807\n",
      "Trainable params: 38,551\n",
      "Non-trainable params: 38,256\n",
      "_________________________________________________________________\n",
      "Epoch 1/2\n",
      "2663/2663 [==============================] - 7s 2ms/step - loss: 0.2071 - accuracy: 0.9544 - val_loss: 0.1262 - val_accuracy: 0.9500\n",
      "Epoch 2/2\n",
      "2663/2663 [==============================] - 6s 2ms/step - loss: 0.2260 - accuracy: 0.9606 - val_loss: 0.0886 - val_accuracy: 0.9670\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\Tom\\AppData\\Local\\Temp\\tmpihf4tu96\\assets\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\Tom\\AppData\\Local\\Temp\\tmpihf4tu96\\assets\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\Tom\\AppData\\Local\\Temp\\tmpvaxs00v4\\assets\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\Tom\\AppData\\Local\\Temp\\tmpvaxs00v4\\assets\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "model_for_pruning = tf.keras.models.clone_model(test_mlp)\n",
    "model_for_pruning.set_weights(test_mlp.get_weights())\n",
    "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
    "\n",
    "# Compute end step to finish pruning after 2 epochs.\n",
    "batch_size = 128\n",
    "epochs = 2\n",
    "validation_split = 0.2 # 20% of training set will be used for validation set. \n",
    "\n",
    "n_samples = X_train.shape[0] * (1 - validation_split)\n",
    "end_step = np.ceil(num_samples / batch_size).astype(np.int32) * epochs\n",
    "\n",
    "# Define model for pruning.\n",
    "pruning_params = {\n",
    "      'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.50,\n",
    "                                                               final_sparsity=0.80,\n",
    "                                                               begin_step=0,\n",
    "                                                               end_step=end_step)\n",
    "}\n",
    "\n",
    "model_for_pruning = prune_low_magnitude(model_for_pruning, **pruning_params)\n",
    "\n",
    "# `prune_low_magnitude` requires a recompile.\n",
    "model_for_pruning.compile(optimizer = 'adam', loss = tf.keras.losses.BinaryCrossentropy(from_logits = True), metrics=['accuracy'])\n",
    "model_for_pruning.summary()\n",
    "\n",
    "callbacks = [\n",
    "  tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "]\n",
    "\n",
    "model_for_pruning.fit(X_train, y_train, epochs = epochs, \n",
    "              validation_split = validation_split, callbacks = callbacks)\n",
    "\n",
    "pruned_model = tfmot.sparsity.keras.strip_pruning(model_for_pruning)\n",
    "pruned_quantized = tf.keras.models.clone_model(pruned_model)\n",
    "pruned_quantized.set_weights(pruned_model.get_weights())\n",
    "\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(pruned_model) # path to the SavedModel directory\n",
    "pruned_model = converter.convert()\n",
    "# Save the model.\n",
    "with open('models/pruned.tflite', 'wb') as f:\n",
    "  f.write(pruned_model)\n",
    "\n",
    "\n",
    "# combined pruning and quantization\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(pruned_quantized) # path to the SavedModel directory\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset\n",
    "pruned_quantized = converter.convert()\n",
    "with open('models/pruned_quantized.tflite', 'wb') as f:\n",
    "  f.write(pruned_quantized)"
   ]
  }
 ]
}